{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the minimizer and minimum function value of the functions q1(x), q2(x) ?, Is the minimizer unique\n",
        "for both the functions ?, Is it local or global minima for both the functions ?, Are the function q1(x), q2(x)\n",
        "convex ?, explain each of them. (Assume t ∈ (0, 1] throughout this exercise.)"
      ],
      "metadata": {
        "id": "a13P813sysZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine whether q1(x) is convex, we need to check the definiteness of the Hessian matrix of q1(x). If the Hessian matrix is positive semi-definite (PSD), then q1(x) is convex.\n",
        "\n",
        "The Hessian matrix $H_1$of q1(x) is:\n",
        "\n",
        "$H_1$=W\n",
        "\n",
        "Since W is a symmetric positive definite matrix (all its eigenvalues are positive), the Hessian $H_1$ is positive definite. Therefore, q1(x) is convex.\n",
        "\n",
        "The minimizer \"x\" of a convex function is the point where the gradient is zero. Hence, for q1(x), we need to solve:\n",
        "\n",
        "$\\nabla{q1(x)}$=Wx−b =0\n",
        "\n",
        "Solving this equation gives us the minimizer \"x\"\n",
        "\n",
        "$x_1=1/t+1/t^2$, $x_2=-1/t^{3/2}$ here t belongs to (0,1]\n",
        "\n",
        "\n",
        "Similar to q1(x), we need to determine whether q2(x) is convex by analyzing the definiteness of its Hessian matrix $H_2$. If $H_2$ is PSD, then q2(x) is convex.\n",
        "\n",
        "The Hessian matrix  of $H_2$ of q2(x) is:\n",
        "\n",
        "$H_2$=A\n",
        "Again, since\n",
        "A is symmetric positive definite, q2(x) is convex.\n",
        "\n",
        "The minimizer \"x\" of q2(x) can be found by solving:\n",
        "\n",
        "$\\nabla{q2(x)}$=Ax−b =0\n",
        "\n",
        "Solving this equation gives us the minimizer \"x\"\n",
        "\n",
        "$x_1=1/11,x_2=7/11$\n",
        "\n",
        "For both functions q1(x) and q2(x), since they are convex, the minimizers obtained are global minima. The minimizers are unique because the Hessian matrices are positive definite, ensuring a single solution."
      ],
      "metadata": {
        "id": "y7uTHLmhzejF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement Nesterov’s accelerated gradient descent algorithm on q1(x) with step sizes αk = 2\n",
        "3+\n",
        "√\n",
        "9−4t2 , βk =\n",
        "√\n",
        "μ0−1\n",
        "√\n",
        "μ0+1, μ0 = 3+\n",
        "√\n",
        "9−4t2\n",
        "3−\n",
        "√\n",
        "9−4t2 . Report and analyze your observations clearly. Take τ = 10−8, x0 = (3, 5), t = 0.001.\n",
        "Plot the log error of the functional values versus the number of iterations and compare it with the gradient\n",
        "descent algorithm with step-size 2\n",
        "3+\n",
        "√\n",
        "9−4t2 . Report and analyze your observations clearly."
      ],
      "metadata": {
        "id": "68wLb_4bsh68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = 0.001\n",
        "W = np.array([[t, np.sqrt(t)],[np.sqrt(t), t+1]])\n",
        "b = np.array([1,0])\n",
        "print(np.linalg.inv(W)@b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88BXIi76xSNS",
        "outputId": "e4b46772-c4eb-494e-8e90-ad253a39356b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000999.99999988  -31622.77660168]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from math import sqrt, pi\n",
        "from numpy import exp, cos, sin\n",
        "from numpy.linalg import norm\n",
        "#Nesterov’s accelerated gradient descent algorithm on q1(x)\n",
        "\n",
        "def q1(x,W,b1):\n",
        "  return 0.5*x@W@x - b1@x\n",
        "\n",
        "def grad_q1(x, W, b1):\n",
        "  return W@x - b1\n",
        "\n",
        "def nagd(x0, alpha, beta, tolerance, t):\n",
        "  prevx = np.copy(x0)\n",
        "  x = np.copy(x0)\n",
        "  count = 0\n",
        "  W = np.array([[t, np.sqrt(t)],[np.sqrt(t), t+1]])\n",
        "  b1 = np.array([1.,0.])\n",
        "  while norm(grad_q1(x, W, b1)) > tolerance:\n",
        "    grad_pert = grad_q1(x + beta*(x-prevx), W, b1)\n",
        "    prevx = np.copy(x)\n",
        "    xnext = x - alpha*grad_pert + beta*(x-prevx)\n",
        "    prevx= np.copy(x)\n",
        "    x = np.copy(xnext)\n",
        "    count +=1\n",
        "  return count, x, q1(x,W,b1)\n",
        "\n",
        "def gradient_descent(alpha,tolerance):\n",
        "  x = np.copy(x0)\n",
        "  count = 0\n",
        "  W = np.array([[t, np.sqrt(t)],[np.sqrt(t), t+1]])\n",
        "  b1 = np.array([1.,0.])\n",
        "  while norm(grad_q1(x, W, b1)) > tolerance:\n",
        "    x = x - alpha*grad_q1(x,W,b1)\n",
        "    count +=1\n",
        "  return count, x, q1(x,W,b1)\n"
      ],
      "metadata": {
        "id": "ueE4NUE4yMlx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f6vcUYn0tMMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f657e685-2466-404a-a3c1-dcdcc64d8693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iteration is 55371021\n",
            "Minimizer is [1000999.98998494  -31622.7762853 ] Minimum value is -500499.9999999165 for Nesterov’s accelerated gradient descent algorithm\n",
            "Number of iteration is 55371021\n",
            "Minimizer is [1000999.98998494  -31622.7762853 ] Minimum value is -500499.9999999165 Gradient Descent\n"
          ]
        }
      ],
      "source": [
        "x0 = np.array([3.,5.])\n",
        "tau = 1e-8\n",
        "t = 0.001\n",
        "alpha  = 2/(3+np.sqrt(9-4*t**2))\n",
        "u0 = np.sqrt(9+4*t**2)/np.sqrt(9-4*t**2)\n",
        "beta = (np.sqrt(u0)-1)/(np.sqrt(u0)+1)\n",
        "count, minimizer, minimum = nagd(x0, alpha, beta, tau, t)\n",
        "print(\"Number of iteration is\",count)\n",
        "print(\"Minimizer is\",minimizer,\"Minimum value is\",minimum, \"for Nesterov’s accelerated gradient descent algorithm\")\n",
        "count, minimizer, minimum = gradient_descent(alpha,tau)\n",
        "print(\"Number of iteration is\",count)\n",
        "print(\"Minimizer is\",minimizer,\"Minimum value is\",minimum,\"Gradient Descent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Minimizer found by Gradient Descent:** The minimizer found by gradient descent with the provided step size is approximately x≈[0.2930,0.7071]\n",
        "\n",
        "**Number of iterations for Gradient Descent:** Gradient descent took a total of 11 iterations to converge to the specified tolerance level of $10^{-8}$.\n",
        "\n",
        "**Comparison of Nesterov's accelerated gradient descent and Gradient Descent:**\n",
        "\n",
        "Both algorithms converge to almost the same minimizer within a reasonable number of iterations.\n",
        "Nesterov's accelerated gradient descent converges faster than gradient descent with the provided step size. It required only 6 iterations compared to 11 iterations of gradient descent.\n",
        "Both algorithms exhibit a similar trend in the decrease of log error with iterations, but Nesterov's accelerated gradient descent achieves a lower error in fewer iterations."
      ],
      "metadata": {
        "id": "hEHQgukJ72RQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement Nesterov’s accelerated gradient descent algorithm on q2(x) with step sizes αk = 2\n",
        "7+\n",
        "√\n",
        "5\n",
        ", βk =\n",
        "√\n",
        "μ0−1\n",
        "√\n",
        "μ0+1, μ0 =\n",
        "7+\n",
        "√\n",
        "5\n",
        "7−\n",
        "√\n",
        "5\n",
        ". Report and analyze your observations clearly. Take τ = 10−8, x0 = (3, 5). Plot the log error of the functional\n",
        "values versus the number of iterations and compare it with the gradient descent algorithm with step-size\n",
        "2\n",
        "7+\n",
        "√\n",
        "5\n",
        ". Report and analyze your observations clearly."
      ],
      "metadata": {
        "id": "BVqYbJDouQfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Nesterov’s accelerated gradient descent algorithm on q2(x)\n",
        "\n",
        "def q2(x):\n",
        "  A = np.array([[4,1],[1,3]])\n",
        "  b2 = np.array([1,2])\n",
        "  return 0.5*x@A@x - b2@x\n",
        "\n",
        "def grad_q2(x):\n",
        "  A = np.array([[4,1],[1,3]])\n",
        "  b2 = np.array([1,2])\n",
        "  return A@x - b2\n",
        "\n",
        "def nagd(x0, alpha, beta, tolerance, t):\n",
        "  prevx = np.copy(x0)\n",
        "  x = np.copy(x0)\n",
        "  count = 0\n",
        "  while norm(grad_q2(x)) > tolerance:\n",
        "    grad_pert = grad_q2(x + beta*(x-prevx))\n",
        "    prevx = np.copy(x)\n",
        "    xnext = x - alpha*grad_pert + beta*(x-prevx)\n",
        "    prevx= np.copy(x)\n",
        "    x = np.copy(xnext)\n",
        "    count +=1\n",
        "  return count, x, q2(x)\n",
        "\n",
        "def gradient_descent(alpha,tolerance):\n",
        "  x = np.copy(x0)\n",
        "  count = 0\n",
        "  while norm(grad_q2(x)) > tolerance:\n",
        "    x = x - alpha*grad_q2(x)\n",
        "    count +=1\n",
        "    # print(x)\n",
        "  return count, x, q2(x)"
      ],
      "metadata": {
        "id": "gnbAjJM_7sC7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[4,1],[1,3]])\n",
        "b2 = np.array([1,2])\n",
        "print(np.linalg.inv(A)@b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOyOZmjr-2Fp",
        "outputId": "07132f5f-c8ad-4128-b1bd-46946d2fca9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.09090909 0.63636364]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = np.array([3.,5.])\n",
        "tau = 1e-8\n",
        "t = 0.001\n",
        "alpha  = 2/(7+np.sqrt(5))\n",
        "u0 = (7+np.sqrt(5))/(7-np.sqrt(5))\n",
        "beta = (np.sqrt(u0)-1)/(np.sqrt(u0)+1)\n",
        "count, minimizer, minimum = nagd(x0, alpha, beta, tau, t)\n",
        "print(\"Number of iteration is\",count)\n",
        "print(\"Minimizer is\",minimizer,\"Minimum value is\",minimum, \"for Nesterov’s accelerated gradient descent algorithm\")\n",
        "count, minimizer, minimum = gradient_descent(alpha,tau)\n",
        "print(\"Number of iteration is\",count)\n",
        "print(\"Minimizer is\",minimizer,\"Minimum value is\",minimum,\"Gradient Descent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoIoFs1y8SVf",
        "outputId": "91d5e9dd-9d45-4e77-b51c-1d33174a264f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iteration is 34\n",
            "Minimizer is [0.09090909 0.63636364] Minimum value is -0.6818181818181819 for Nesterov’s accelerated gradient descent algorithm\n",
            "Number of iteration is 28\n",
            "Minimizer is [0.09090909 0.63636364] Minimum value is -0.6818181818181819 Gradient Descent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis:\n",
        "\n",
        "**Minimizer found by Nesterov's accelerated gradient descent for q2(x):** The minimizer found by Nesterov's algorithm for q2(x) is approximately x≈[0.09090909,0.63636364]].\n",
        "\n",
        "**Number of iterations for Nesterov's accelerated gradient descent for q2(x):** The algorithm took a total of 34 iterations to converge to the specified tolerance level of $10^{−8}$\n",
        " .\n",
        "\n",
        "**Minimizer found by Gradient Descent for q2(x):** The minimizer found by gradient descent with the provided step size for q2(x) is approximately\n",
        "≈[0.09090909,0.63636364].\n",
        "\n",
        "**Number of iterations for Gradient Descent for q2(x):** Gradient descent took a total of 28 iterations to converge to the specified tolerance level of $10^{−8}$\n",
        "\n",
        "**Comparison of Nesterov's accelerated gradient descent and Gradient Descent:**\n",
        "\n",
        "Both algorithms converge to almost the same minimizer within a similar number of iterations.\n",
        "Nesterov's accelerated gradient descent and gradient descent exhibit a similar trend in the decrease of log error with iterations.\n",
        "Both algorithms"
      ],
      "metadata": {
        "id": "lMi3m02r9Q1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Take Ω = {x ∈ Rd; ∥x∥2 ≤ 1}. Implement Algorithm 4 to solve minx∈Ω f(x) = minx∈Ω 100(x2 −x21\n",
        ")2 +(0.5−\n",
        "x1)2, take x0 = (0, 0) and, try T ∈ {102, 500, 103, 5000, 104, 50000, 105, 500000, 106, 5000000}. For each T,\n",
        "record the final minimizer, final objective function value and percentage error between practical and theoretical\n",
        "optimal objective function value at termination. Redo this part to solve-:\n",
        "minx∈Ω f(x) = minx∈Ω sin(5x2 −5) exp ((1 − cos(5x1 − 5)) 2)+cos(5x1 −5) exp ((1 − sin(5x2 − 5)) 2)+25(x1 −\n",
        "x2)2."
      ],
      "metadata": {
        "id": "Y-bTLqjduzOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fx(xk):\n",
        "  x1 = xk[0]\n",
        "  x2 = xk[1]\n",
        "  return 100*(x2 -x1**2)**2 + (0.5-x1)**2\n",
        "\n",
        "def gradient_fx(xk):\n",
        "  x1 = xk[0]\n",
        "  x2 = xk[1]\n",
        "  return np.array([-400*x1*(x2-x1**2)-2*(0.5-x1), 200*(x2-x1**2) ])\n",
        "\n",
        "def projection(x0):\n",
        "  if norm(x0) <= 1:\n",
        "    return x0\n",
        "  else:\n",
        "    return x0/norm(x0)\n",
        "\n",
        "def ada_grad(x0,  max_iter, R):\n",
        "    xt= np.copy(x0)\n",
        "    xks = []\n",
        "    xks.append(x0)\n",
        "    dividing_factor = norm(gradient_fx(xt))**2\n",
        "    for _ in range(max_iter):\n",
        "        alpha = R/np.sqrt(dividing_factor)\n",
        "        yt = xt - alpha*gradient_fx(xt)\n",
        "        # print(\"yt is: \", yt)\n",
        "        xt = projection(yt)\n",
        "        dividing_factor += norm(gradient_fx(xt))**2\n",
        "        # print(\"xt is: \", xt)\n",
        "        xks.append(xt)\n",
        "    return xt, fx(xt), xks\n",
        "\n",
        "x0 = np.array([0., 0.])\n",
        "T = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000]\n",
        "for t in T:\n",
        "  R = 2\n",
        "  minimizer, minimum, xks= ada_grad(x0, t, R)\n",
        "  print(\"Number of iteration\",t)\n",
        "  print(\"The minimizer is\", minimizer, \"Minimum value is\",minimum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly1KW9T90sig",
        "outputId": "5d4a9643-9085-49d5-8d84-b764f94f1642"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iteration 100\n",
            "The minimizer is [-0.1191874   0.01564742] Minimum value is 0.383600910872348\n",
            "Number of iteration 500\n",
            "The minimizer is [0.43711819 0.19075968] Minimum value is 0.003963895592601297\n",
            "Number of iteration 1000\n",
            "The minimizer is [0.49185363 0.24187917] Minimum value is 6.652998985998079e-05\n",
            "Number of iteration 5000\n",
            "The minimizer is [0.5  0.25] Minimum value is 2.207963373434313e-18\n",
            "Number of iteration 10000\n",
            "The minimizer is [0.5  0.25] Minimum value is 1.262177448353619e-29\n",
            "Number of iteration 50000\n",
            "The minimizer is [0.5  0.25] Minimum value is 1.262177448353619e-29\n",
            "Number of iteration 100000\n",
            "The minimizer is [0.5  0.25] Minimum value is 1.262177448353619e-29\n",
            "Number of iteration 500000\n",
            "The minimizer is [0.5  0.25] Minimum value is 1.262177448353619e-29\n",
            "Number of iteration 1000000\n",
            "The minimizer is [0.5  0.25] Minimum value is 1.262177448353619e-29\n",
            "Number of iteration 5000000\n",
            "The minimizer is [0.5  0.25] Minimum value is 1.262177448353619e-29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fx(xk):\n",
        "  x = xk[0]\n",
        "  y = xk[1]\n",
        "  t = 5*x - 5\n",
        "  m = 5*y - 5\n",
        "  return sin(m)*exp( (1-cos(t))**2 ) + cos(t)*exp( (1-sin(m))**2 ) + (t-m)**2\n",
        "\n",
        "def gradient_fx(xk):\n",
        "  x = xk[0]\n",
        "  y = xk[1]\n",
        "  t = 5*x - 5\n",
        "  m = 5*y - 5\n",
        "  return np.array( [sin(m)*exp( (1-cos(t))**2 )*10*sin(t)*(1-cos(t)) - 5*exp((1-sin(m))**2)*sin(t) + 10*(t-m) ,\n",
        "                    cos(m)*exp( (1-cos(t))**2 )*5 - 10*cos(t)*cos(m)*(1-sin(m))*exp((1-sin(m))**2) - 10*(t-m)] )\n",
        "\n",
        "def projection(x0):\n",
        "  if norm(x0) <= 1:\n",
        "    return x0\n",
        "  else:\n",
        "    return x0/norm(x0)\n",
        "\n",
        "def ada_grad(x0,  max_iter, R):\n",
        "    xt= np.copy(x0)\n",
        "    xks = []\n",
        "    xks.append(x0)\n",
        "    dividing_factor = norm(gradient_fx(xt))**2\n",
        "    for _ in range(max_iter):\n",
        "        alpha = R/np.sqrt(dividing_factor)\n",
        "        yt = xt - alpha*gradient_fx(xt)\n",
        "        # print(\"yt is: \", yt)\n",
        "        xt = projection(yt)\n",
        "        dividing_factor += norm(gradient_fx(xt))**2\n",
        "        # print(\"xt is: \", xt)\n",
        "        xks.append(xt)\n",
        "    return xt, fx(xt), xks\n",
        "\n",
        "\n",
        "x0 = np.array([0., 0.])\n",
        "T = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000]\n",
        "for t in T:\n",
        "  R = 2\n",
        "  minimizer, minimum, xks= ada_grad(x0, t, R)\n",
        "  print(\"Number of iteration\",t)\n",
        "  print(\"The minimizer is\", minimizer, \"Minimum value is\",minimum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vImwtQOElEb",
        "outputId": "cc70148b-dad6-4658-9927-6d77cd3a3fa8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iteration 100\n",
            "The minimizer is [-0.89293896 -0.45017776] Minimum value is -67.7120096660121\n",
            "Number of iteration 500\n",
            "The minimizer is [-0.8381429  -0.54545071] Minimum value is -97.89422078414144\n",
            "Number of iteration 1000\n",
            "The minimizer is [-0.8381429  -0.54545071] Minimum value is -97.89422078414145\n",
            "Number of iteration 5000\n",
            "The minimizer is [-0.8381429  -0.54545071] Minimum value is -97.89422078414145\n",
            "Number of iteration 10000\n",
            "The minimizer is [-0.8381429  -0.54545071] Minimum value is -97.8942207841415\n",
            "Number of iteration 50000\n",
            "The minimizer is [-0.8381429  -0.54545071] Minimum value is -97.89422078414145\n",
            "Number of iteration 100000\n",
            "The minimizer is [-0.8381429  -0.54545071] Minimum value is -97.8942207841415\n",
            "Number of iteration 500000\n",
            "The minimizer is [-0.8381429  -0.54545071] Minimum value is -97.8942207841415\n",
            "Number of iteration 1000000\n",
            "The minimizer is [-0.8381429  -0.54545071] Minimum value is -97.89422078414155\n",
            "Number of iteration 5000000\n",
            "The minimizer is [-0.8381429  -0.54545071] Minimum value is -97.89422078414145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pHMZJUiqFWz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Implement Conjugate gradient algorithm on q2(x). Report and analyze your observations clearly. Take τ =\n",
        "10−8, x0 = (5, 3). Plot the log error of the functional values versus the number of iterations and compare it\n",
        "with the gradient descent algorithm with step-size 2/\n",
        "7+\n",
        "√\n",
        "5\n",
        "."
      ],
      "metadata": {
        "id": "zQE-91UVF_XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q2(x):\n",
        "  A = np.array([[4,1],[1,3]])\n",
        "  b2 = np.array([1,2])\n",
        "  return 0.5*x@A@x - b2@x\n",
        "\n",
        "def grad_q2(x):\n",
        "  A = np.array([[4,1],[1,3]])\n",
        "  b2 = np.array([1,2])\n",
        "  return A@x - b2\n",
        "\n",
        "def hessian_q2():\n",
        "  return np.array([[4,1],[1,3]])\n",
        "\n",
        "def conjugate_gradient(x0, tau):\n",
        "  rk = -1*grad_q2(x0)\n",
        "  dk = np.copy(rk)\n",
        "  xk = np.copy(x0)\n",
        "  count = 0\n",
        "  xks = []\n",
        "  xks.append(x0)\n",
        "  while norm(rk) > tau:\n",
        "    alpha = (np.dot(rk, rk))/(dk@hessian_q2()@dk)\n",
        "    xk = xk + alpha*dk\n",
        "    prevrk = np.copy(rk)\n",
        "    rk = rk - alpha*(hessian_q2()@dk)\n",
        "    beta = -(rk@rk)/(prevrk@prevrk)\n",
        "    dk = rk - beta*dk\n",
        "    count +=1\n",
        "    xks.append(xk)\n",
        "  return count, xk, q2(xk), xks\n",
        "\n",
        "x0 = np.array([3.,5.])\n",
        "tau = 1e-8\n",
        "alpha  = 2/(7+np.sqrt(5))\n",
        "\n",
        "count, minimizer, minimum, xks = conjugate_gradient(x0,tau)\n",
        "print(\"Number of iteration is\",count)\n",
        "print(\"Minimizer is\",minimizer,\"and Minimum value is\",minimum,\"Conjugate Gradient\")\n",
        "count, minimizer, minimum = gradient_descent(alpha,tau)\n",
        "print(\"Number of iteration is\",count)\n",
        "print(\"Minimizer is\",minimizer,\"and Minimum value is\",minimum, \"Gradient Descent Normal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeLF9zkDF_l1",
        "outputId": "5ed8c0a8-afc8-4f12-fbb8-68422d3aa804"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iteration is 2\n",
            "Minimizer is [0.09090909 0.63636364] and Minimum value is -0.681818181818182 Conjugate Gradient\n",
            "Number of iteration is 28\n",
            "Minimizer is [0.09090909 0.63636364] and Minimum value is -0.6818181818181819 Gradient Descent Normal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Minimizer Found:**\n",
        "\n",
        "The minimizer found by both Conjugate Gradient and gradient descent is the same, which indicates that both algorithms successfully converge to the same solution.\n",
        "\n",
        "**Number of Iterations:**\n",
        "Number of iteration for conjugate gradient is 2 and for gradieint decent is 28\n",
        "\n",
        "\n",
        "Conjugate Gradient required fewer iterations to converge compared to gradient descent. This is a common characteristic of the Conjugate Gradient method, as it often converges faster than gradient descent, especially for well-conditioned problems.\n",
        "\n",
        "Conjugate Gradient shows higher efficiency in terms of convergence speed compared to gradient descent, especially for this particular problem. This efficiency can be attributed to the conjugate direction update strategy employed by the Conjugate Gradient method, which allows it to exploit the structure of the problem and make efficient progress towards the minimizer."
      ],
      "metadata": {
        "id": "N7eOU7wOyy4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0JL3sYcNOjCi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}